{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Running memristor model using weights and input of HAR model using pytorch and vivado\n",
        "in this program i have used human activity recognition(HAR) dataset, in previously i have used same dataset to find the important feature from the same dataset where i have used SHAP XAI technique where i have by increased and decred the feature and got different accuracy so have used 95 feature where accuracy is near to by using all 561 feature.\n",
        "\n",
        "i have used this reference paper \"An FPGA-based memristor emulator for artificial neural network\" where they used a MNIST dataset,\n",
        "step 1: used MNIST dataset trained and tested using single layer perceptron(SLP).\n",
        "step 2: used the weights of SLP and quantized between (0-2) with step size = 0.25 with 8-valued memristor. then converted to fixed point number which can be used in verilog when running descrete memristor model.\n",
        "step 3: the features are scaled between (-4, 4) then converted to fixed point number where input and weights can be expressed in 4 digits for integer, 2 digits for decimal and 1 digit for sign.\n",
        "\n",
        "\n",
        "i have replicated the same paper i have 1st used the same dataset and same procedure later chnaged the dataset to HAR used 95 feature instead 561 feature. then trained and tested using single layer perceptron (SLP) saved the best model and their weights later followed above step 1, 2 and 3. converted to .txt file so that can be uploaded while writing a program for memrristor model.  \n",
        "\n",
        "memristor model: as mentioned in the paper if consider MNIST dataset there are 784 inputs (28x28 image pixel) and there are 10 handwritten digit so 784*10=7840 weights and each pixel has one memristor so there are 7840 memristor module.\n",
        "\n",
        "mow in this i have used HAR dataset in that 95 feature so there are 95 inputs and there are 6 class (0-5) so 95*6=570 memristor module."
      ],
      "metadata": {
        "id": "JMt62CaQFPMq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5AT3C9WE0XQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        
        "# Load and split HAR dataset (features + class)\n",
        
        "X_train_full = pd.read_csv(\"/content/X_train_reduced_95(1).csv\", header=0)\n",
        "X_test_full  = pd.read_csv(\"/content/X_test_reduced_95(1).csv\", header=0)\n",
        "\n",
        "X_train = X_train_full.iloc[:, :-1]\n",
        "y_train = X_train_full.iloc[:, -1].astype(int) - 1\n",
        "\n",
        "X_test = X_test_full.iloc[:, :-1]\n",
        "y_test  = X_test_full.iloc[:, -1].astype(int) - 1\n",
        "\n",
        
        "#  Scale inputs to (-4, 4)\n",
        
        "X_min = X_train.min()\n",
        "X_max = X_train.max()\n",
        "\n",
        "X_train_scaled = ((X_train - X_min) / (X_max - X_min)) * 8 - 4\n",
        "X_test_scaled  = ((X_test  - X_min) / (X_max - X_min)) * 8 - 4\n",
        "\n",
       
        "#  Convert to tensors for PyTorch training\n",
        
        "X_train_t = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "\n",
        "X_test_t = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=128, shuffle=False)\n",
        "\n",
        
        "#  Define Single-Layer Perceptron (SLP)\n",
        
        "class SLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(SLP, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        
        "# Train the SLP\n",
        
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SLP(input_dim=X_train.shape[1], num_classes=6).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y.size(0)\n",
        "        correct += (predicted == y).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/10] | Loss: {total_loss/len(train_loader):.4f} | Accuracy: {acc:.2f}%\")\n",
        "\n",
        
        "# Evaluate model on test set\n",
       
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        outputs = model(X)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y.size(0)\n",
        "        correct += (predicted == y).sum().item()\n",
        "\n",
        "print(f\"âœ… Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
       
        "# Scale weights (0 â†’ 2) and quantize to Q4.2\n",
        
        "weights = model.fc.weight.data.cpu().numpy()\n",
        "\n",
        "# Scale weights to [0, 2]\n",
        "# Scale weights to [0, 2]\n",
        "w_min, w_max = weights.min(), weights.max()\n",
        "weights_scaled = ((weights - w_min) / (w_max - w_min)) * 2\n",
        "\n",
        "# Quantize to Q4.2 (Ã—4)\n",
        "weights_fixed = (weights_scaled * 4).round().astype(int).clip(0, 8)\n",
        "\n",
        "# Save weights\n",
        "pd.DataFrame(weights_fixed).to_csv(\"weights_HAR_fixed.csv\", index=False, header=False)\n",
        "\n",
        
        "# Quantize and save scaled inputs (X_train, X_test)\n",
       
        "def quantize_and_save(X_scaled, y, name):\n",
        "    X_fixed = (X_scaled * 4).round().astype(int).clip(-16, 16)\n",
        "    df_fixed = pd.concat([pd.DataFrame(X_fixed), pd.Series(y.values)], axis=1)\n",
        "    df_fixed.to_csv(f\"{name}_fixed.csv\", index=False, header=False)\n",
        "    print(f\"âœ… Saved: {name}_fixed.csv\")\n",
        "\n",
        "quantize_and_save(X_train_scaled, y_train, \"HAR_train\")\n",
        "quantize_and_save(X_test_scaled,  y_test,  \"HAR_test\")\n",
        "\n",
        "print(\"ðŸŽ‰ All fixed-point datasets and weights saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        
        "# Save raw, scaled, and fixed-point weights\n",
       
        "# 7.1 Get raw learned weights\n",
        "weights = model.fc.weight.data.cpu().numpy()  # shape: [num_classes, num_features]\n",
        "pd.DataFrame(weights).to_csv(\"weights_HAR_raw.csv\", index=False, header=False)\n",
        "print(\"Saved: weights_HAR_raw.csv (original learned weights)\")\n",
        "\n",
        "# 7.2 Scale weights to range (0 â†’ 2)\n",
        "w_min, w_max = weights.min(), weights.max()\n",
        "weights_scaled = ((weights - w_min) / (w_max - w_min)) * 2\n",
        "pd.DataFrame(weights_scaled).to_csv(\"weights_HAR_scaled_0to2.csv\", index=False, header=False)\n",
        "print(\" Saved: weights_HAR_scaled_0to2.csv (scaled to 0â€“2)\")\n",
        "\n",
        "# 7.3 Quantize to Q4.2 fixed-point (Ã—4)\n",
        "weights_fixed = (weights_scaled * 4).round().astype(int).clip(0, 8)\n",
        "pd.DataFrame(weights_fixed).to_csv(\"weights_HAR_fixed.csv\", index=False, header=False)\n",
        "print(\" Saved: weights_HAR_fixed.csv (Q4.2 integers, 0â€“8)\")\n",
        "\n",
        "# 7.4 Optional: Save as one-value-per-line .txt file (for Verilog)\n",
        "weights_fixed_flat = weights_fixed.flatten()\n",
        "np.savetxt(\"weights_HAR_fixed1.txt\", weights_fixed_flat, fmt=\"%d\")\n",
        "print(\" Saved: weights_HAR_fixed1.txt (one integer per line, ready for Verilog)\")"
      ],
      "metadata": {
        "id": "fZZlaQlvE-sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code snippet for extraing single row feature for testing in vivado."
      ],
      "metadata": {
        "id": "VXOXKIsNRe9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load fixed-point test set (features + label)\n",
        "df = pd.read_csv(\"/content/HAR_test_fixed.csv\", header=None)\n",
        "\n",
        "row_index = 1316   # Python is 0-based â†’ row 3\n",
        "sample = df.iloc[row_index, :-1]   # exclude label column\n",
        "\n",
        "np.savetxt(\"X_test_row1316.txt\", sample.values, fmt=\"%d\")\n",
        "print(\" Saved 3rd test sample to X_test_row1316.txt\")"
      ],
      "metadata": {
        "id": "_a0y1evyFK6i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
